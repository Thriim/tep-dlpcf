\documentclass{beamer}
 
\usepackage[utf8]{inputenc} 
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{graphicx}
\usepackage[english]{babel}
\usepackage{alltt}

\usepackage{amssymb}
%% \usepackage{fullpage}
\usepackage{bussproofs}
\usepackage[utf8]{inputenc}
%% \usepackage[english]{babel}
\usepackage{verbatim}
\usepackage{mathabx}

\usetheme{Warsaw}

\title{Linear Dependent Types in a Call-by-Value scenario}
\subtitle{Synthesis}
\author{I\~{n}igo Mediavilla \& Pierrick Couderc}
\date{October 28, 2013}

\begin{document}

\begin{frame}
  \maketitle
\end{frame}

\begin{frame}[fragile]
\frametitle{Classical Type systems don't assure termination}


\begin{verbatim}
add = fix f y z. ifz y then z else (succ (f (pred y) z))
\end{verbatim}

\begin{verbatim}
divergef = fix f y z. ifz y then z else (succ (f y z))
\end{verbatim}
Both have type Nat $\rightarrow$ Nat but one diverges and the other doesn't
\end{frame}

\begin{frame}[fragile]
  \frametitle{Classical type systems don't inform about complexity}
  Which one is best? 
  \begin{alltt}
    identity = \(\lambda\)x. x 
  \end{alltt}
  \begin{alltt}
    madIdentity = fix f x. 
                    ifz x then 0 else (succ (f (pred x))
  \end{alltt}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Two improvements}
  \begin{itemize}
  \item Dependent types
    \begin{figure}
\begin{alltt}
 add : Nat[a] \(\rightarrow\) Nat[b] \(\rightarrow\) Nat[a+b]
\end{alltt}
    \end{figure}
  \item Linear types \\
    \begin{figure}
      \begin{center}
        $!_{n} \sigma ~\multimap~\tau$
      \end{center}
    \end{figure}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Putting them together}
  \begin{itemize}
  \item Dependent types provide information about the function's inputs and outputs

  \item Linear types provide information about how many times a parameter is
    copied or a function is used (substitution)

  \item Taking the information from both + giving an extra context
    ($\varepsilon$) allows to infer a function's complexity and thus termination
  \end{itemize}

  \medskip

  The result gives \textbf{d$l$PCF}.
\end{frame}

\begin{frame}[fragile]
  \frametitle{A simple example - Informally}
\begin{verbatim}
  add = fix f y z. ifz y then z else (succ (f (pred y) z))
\end{verbatim}
\begin{itemize}


\item  How many substitutions will there be? 
  
  if: $t = \lambda~y~z.ifz~y~then~z~else~(succ~(f~(pred~y)~z))$

\item substitutions(add) = recursive-calls + (recursive-calls * substitutions(t))

  % There's "n" reductions for substituting the fixpoint operator
  % + "n * substitutions(t)" reductions

\end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{A simple example - Formally}

  
        \AxiomC{$(b,\phi); (b<H,\Phi); \Gamma, x :[a < I] \cdot A \vdash_{J}^{\varepsilon}$
        $ t : [a<1]\cdot B$}
        \noLine
        \UnaryInfC{$\phi; \Phi \models_{\varepsilon}$ 
          $H \ge \otriangleup_{b}^{0, K} I$}
        \noLine
        \UnaryInfC{$(a,b,\phi); (a < I,b < H, \Phi) \vdash_{\varepsilon}$ 
          $B\{0/a\}\{\otriangleup_{b}^{b+1,a}I + b + 1 + 1/b\} \sqsubseteq A $}
        \noLine
        \UnaryInfC{$(a,\phi); (a < K, \Phi) \vdash_{\varepsilon}$ 
          $B\{0/a\}\{\otriangleup_{b}^{b+1,a}I/b\} \sqsubseteq C $}
        \noLine
        \UnaryInfC{$\phi; \Phi \vdash_{\varepsilon}$ 
          $\Sigma \sqsubseteq \sum_{b<H} \Gamma$}

        \RightLabel{(Rec)}
        \UnaryInfC{$\phi; \Phi; \Sigma \vdash^{\varepsilon}_{H+\sum_{b<H}J}$
          $fix~x.t~:~[a<K]~C$}
        \DisplayProof 

  % Explain only H+\sum_{b<H}J and why it cannot be as simple as H + J * H

\end{frame}

\begin{frame}[fragile]
  \frametitle{A simple example - Execution }

\begin{verbatim}
  add 3 4 = succ(add 2 4)
  -> add 2 4 = succ(add 1 2)
  -> -> add 1 4 = succ(add 0 4)
  -> -> -> add 0 4 = 0
\end{verbatim}
  % Explain formula with the help of the trace
\end{frame}

\begin{frame}
  \frametitle{Compilation in an abstract machine}

  d$l$PCF can be compiled in a CEK abstract machine.

  \medskip

  CEK composition:
  \begin{itemize}
  \item A stack $\pi$.
  \item A closure $c~=~<t;~\varepsilon >$, with $t$ a term and $\varepsilon$ the
    environment, to interact with the stack.
  \end{itemize}

\end{frame}

\end{document}

% Slide 3
Two improvements over classical type systems


The type carries more meaningful information about the transformations
perform inside a function.

e.g

sized arrays => ArrayList<Integer, Size>  

- Linear types

Come from linear logic, allow to  define variables that cannot be shared
(e.g iterator) or that can be shared only "n" times.

e. g

Types define the number of times a variable is "used"

% Slide 4
Putting them together

- Dependent types provide information about the function inputs and outputs

- Linear types provide information about how many times a parameter is
  copied or a function is used (substitution)

- Taking the information from both + giving an extra context (Epsilon) allows to
infer a function's complexity and thus termination


A simple example

  add: fix f y z => ifz y then 0 else (succ (f (pred y) z)

  dependent type: (Nat[a] => (Nat[b] => Nat[a + b]))

  How many substitutions will there be? 

  Informally: substitutions = subst-of-recursive-function +
  (subst-per-recursive-call * number-recursive-calls)

% Slide 5
A simple example (2)

  Epsilon will save us:  

  I (how many recursive calls) = If b < f then 1 else 0

  subst-of-recursive-functon = <>0..f for b of I

  e.g add f=3 g=4

  add 3 4 = succ(add 2 4)
  -> add 2 4 = succ(add 1 2)
  -> -> add 1 4 = succ(add 0 4)
  -> -> -> add 0 4 = 0

  add is substituted 4 times (last time it's substituted even if its not
  called)

A simple example (3)

  number-recursive-calls = subst-of-recursive-function
  
  e.g add =>

  subst-per-recursive-call = 2 = 1 (call to succ) + 1 (call to pred)


  So the final cost is 
  
  cost = 4 + (2 * 4) = (f + 1) + 2 * (f + 1) 

%Slide 6
An abstract machine for PCF

CEK : this abstract machine is really simple and specialized for lambda calculus

There are 2 things to consider : the stack and the term to evaluate, with its
actual environment. 

It's kind of simple : for example, when evaluating an application, the closure
is first push on the stack, and then, since only a value remains, the stakc is
poped and the result is applied to the remaining value.

CEK is used in particular with call-by-value lambda calculus
