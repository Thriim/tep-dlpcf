\documentclass[a4paper,12pt]{article}

\usepackage{amssymb}
\usepackage{bussproofs}
\usepackage[utf8]{inputenc}
%% \usepackage[english]{babel}
\usepackage{verbatim}
\usepackage{mathabx}

\title{Linear Dependent Types in a Call-by-Value Scenario \\
- \\
Synthesis}

\author{I\~{n}igo Mediavilla \and Pierrick COUDERC}

\begin{document}

\maketitle

%% \chapter{Notes}

%% \section{Introduction}

%% \section{Linear Dependent Types, Intuitively}

%% Lambda-calculus in PCF Type System: 
%% \begin{itemize}
%% \item fix: fixpoint operator, for recursive functions
%% \item ifz: if zero
%% \item s: succ
%% \item p: pred
%% \end{itemize}

%% Functions presented:
%% \begin{verbatim}
%% let rec dbl x =
%%   if x = 0 then x
%%   else succ @@ succ @@ dbl @@ pred x

%% let rec div x = (* diverges *)
%%   if x = 0 then x
%%   else succ @@ succ @@ div x
%% \end{verbatim}


%% \chapter{Article}

\paragraph{Abstract} 
Linear dependent types are powerful tools to prove the termination of
programs and evaluate its computational cost. The d$l$PCF type system is an
application of those ideas on Plotkin's PCF, giving a complete langage that can
easily interpreted in a CEK-like abstract machine.

\section{Introduction}

Types and types systems are an extremely useful and powerful tools
that guarantee that a program verifies certain properties, mainly that
it ``cannot go wrong'' (i.e. there shouldn't be types
incompatibilities and crash during the execution), especially with
languages using strong type systems. However, most types systems
aren't capable to catch other properties like termination and
execution time, i.e. the computational complexity. For example, we can
consider this simple factorial function:

\begin{verbatim}
let rec fact n = if n = 0 then 1 else n * fact (n-1)
\end{verbatim}

We can easily prove that such a recursive function will terminate if we consider
only the natural integers, but there is no other way to check it before
executing the function.

The idea of the linear dependent types \cite{ldtrc} is to extend a type system
to add the ability for the typechecker to, given a program, verify that it
terminates, give an approximation of its computational complexity and specify
the semantics of its behavior. The language used will be Plotkin's
PCF with the two following two properties added:

\begin{itemize}
\item \textbf{Linearity} : being able to catch the number of evaluations required
  for a term $t$, this way giving an idea of the computational complexity the
  program.
\item \textbf{Dependency} : the possibility to instantiate distinct copies of a term with
  different types.
\end{itemize}

Such PCF programs, typed with d$l$PCF, can then be interpreted by Krivine's
Abstract Machine for a Call-by-Name evaluation strategy, but this setting is not
really efficient, and few languages are actually using it. The idea
is then to extend the original d$l$PCF (called d$l$PCF$_{N}$) with the ability to
type programs with a call-by-value strategy \cite{ldtcbv}, namely d$l$PCF$_{v}$,
and use the Felleisen and Freidman's CEK machine to interpret it.


\begin{figure}[ht]
  \begin{center}
    $t~::=~n~|~x~|~(\lambda x.t)~|~p(t)~|~s(t)$ \\
    $|~ifz~$t$~then~$u$~else~$v $|~fix~x.t$
  \end{center}
  \caption{Plotkin's PCF's syntax}
  \label{pcf-syntax}
\end{figure}

\begin{figure}[ht]
  \begin{center}
    \begin{tabular}{r c l}
      $(\lambda x.t)~v$ & $\rightarrow$ & $t[v/x]$ \\
      $s(n)$ & $\rightarrow$ & $ n~+~1$ \\
      $p(n+1)$ & $\rightarrow$ & $ n$ \\
      $p(0)$ & $\rightarrow$ & $0$ \\
      $ifz~0~then~u~else~v$ & $\rightarrow$ & $u$ \\
      $ifz~n+1~then~u~else~v$ & $\rightarrow$ & $v$ \\
      $(fix~x.t)~v$ & $\rightarrow$ & $(t[(fix~x.t)/x])~v$
    \end{tabular}
  \end{center}
  \caption{The PCF semantics}
  \label{pcf-sem}
\end{figure}


\section{Introduction to linear and dependent types}

Linear and dependent types are two completely different
concepts. Linear types come from Girard's linear logic
\cite{girard1987linear}, a really powerful extension of classical logic
that expresses well resource manipulation. It does so by controlling
the use of the rules of contraction and weakening, two rules that
don't fit in a system where propositions do not represent unlimited
resources. Limiting those rules changes the interpretation of
propositions like $A \Rightarrow B$ that in classical logic means
``if A is true then I can be sure that B is'' to linear logic
propositions similar to $A!_n \multimap B$ that means ``give me n
elements of A and I can give you a B''. The application of linear logic to type
systems is a type system where the developer can define constraints on the
number of times a variable can be used. As an example, using this kind of
restriction, a concurrent program can define a mutable variable without
the risk of concurreny clashes since the type system ensures that the variable
will only be copied once.

A dependent type is a type that is defined in relation to a value. One can think
of a function that given a value returns a type. That function in the theory of
dependent types is a family of types and the type of that function is a
dependent type. A simple example of dependent types is the arrays of natural
numbers. In most programming languages arrays are parameterized on the kind
of elements the array can contain. In languages with dependent types one can
define a type family with the arrays that carry on their type the size of the
array: i.e. $array(numElements, typeofelements)$. Taking the example of an array of
integers with one element its dependent type would be $oneElementArray:array(1,
integer)$ . With this flexibility, dependent type systems are capable of defining
very expressive types that capture more meaningful restrictions that usual type
systems.


\section{Introduction to D$l$PCF}

First of all, let's consider a simple program written in PCF (whose syntax and
semantics are recalled in Figures \ref{pcf-syntax} and \ref{pcf-sem}) :

\begin{center} 
  dbl = fix~f.$\lambda $x.~ifz~x~then~x~else~s(s(f(p(x)))) 
\end{center}

Simply, this function will return the double of its argument. In the PCF type
system, it will be typed as Nat $\rightarrow$ Nat. However, there is no other
information about its behavior and its meaning in this type. For example,
a lot of unary functions can be typed the same way (an identity function for
example). In particular, if we change p(x) by x, this function will have the
same type, but it will diverge. We need a type system that will be able to
catch that diverging behavior.

As a first step, we can extend the types with information on values,
like for \emph{dbl} : $\tau$ = Nat[a] $\rightarrow$ Nat[2 $\times$
a]. This way, we can see that the type contains the exact behavior of
the function. 

%%the function. However some functions, for example those that are called
%%recursively like \emph{dbl}, will receive an initial parameter that
%%will be altered and passed to the nested recursive calls. The type
%%\emph{Nat[2,4]} is an example of the notation that is used to
%%express that a function is called with values within a range, with 2
%%being the lower bound of the range and 4 being the greater
%%bound.

\medskip

Secondly, a linear dependent type system has the interesting property of
being able to infer the complexity of a given function even it is
recursive. By infering the complexity of a function, the linear
dependent type system is also capable of identifying the functions that
diverge. Using the ideas from linear logic we could, for
example give the following type to dbl :

\begin{center}
$\vdash_{I}$ dbl : Nat $\rightarrow$ Nat
\end{center}

Where I is the cost to compute dbl. If I can be derived then 
 the function doesn't diverge. For the dbl function where
we replace the $p(x)$ by $x$ the type system would realize that the function
doesn't terminate and will show a compilation error.

The idea behind dlPCF merges imprecise types and computational
costs, combined with ideas from bounded linear logic (BLL)
\cite{girard1992bounded}. BLL allows a practical way to
represent the number of times a function uses its argument or it's
substituted (depending on wether we talk about d$l$PCF$_{N}$ or 
d$l$PCF$_{V}$ respectively). As an example the following formula 
in  d$l$PCF$_{V}$:

\begin{center}
$!_{n}\sigma~\multimap~\tau$
\end{center} 

Means that $\sigma$ is substituted $n$ times during evaluation. 

\medskip

Finally, a judgement in d$l$PCF would look this way, for a function from Nat to
Nat :

\begin{center}
$\vdash_{g(a)}~t~:~!_{m}~Nat[a]~\multimap~Nat[f(a)]$ 
\end{center}

In such a type, the function would use its argument $m$ times, returning 
the value of $f$ applied to its argument ($a$ in this case), at a computational
cost of $g(a)$. Actually, d$l$PCF is slightly improved by changing the 
$!_{m}\sigma$ for $[a < m] \cdot \sigma$. That kind of typing still means that 
the parameter will be used m times, but every time the argument is used
the parameter $a$ will take a value in between 0 and $m-1$. The
explanation of why this is useful will clarify when explaining an
example of typing a recursive function later in the article, but 
for the moment we can say that this parameter is used in 
the equational context $\epsilon$ to define first order functions in
terms that allow to calculate the cost of a function. This parametric
definition of the type, has the advantage that it keeps the same skeleton
for all types of $\sigma$.

Some functions are called many times within the same term
Functions that are called
recursively like \emph{dbl}, will receive an initial parameter that
will be altered an passed to the nested recursive calls. To express
this fact D$l$PCF uses type ranges for the dependent types. The type
\emph{Nat[2,4]} is an example of the notation that is used to
express that a function is called with values within a range, with 2
being the lower bound of the range and 4 being the greater
bound.

At last, there is another parameter that can be considered, which is the
$\varepsilon$ class of functions that can be used to tune the type system and
cost of evaluation.

Finally, we can type \emph{dbl} in d$l$PCF:

\begin{center}
$\vdash^{\varepsilon}_{h(a)}~dbl~:~[b~<~a] \cdot Nat[a]~\multimap~Nat[2~\times~a]$
\end{center}

\section{d$l$PCF}


Of course, programs that will be considered will be written in PCF, which is an
extension of the $\lambda$-calculus. 

More formally, the types will be defined as follows:

\begin{center}
  \begin{tabular}{l c r}
    $\sigma, \tau~::=~Nat[I,J]~|~A~\multimap~\sigma$ & ~~~~ & (basic types) \\
    $A, B~::=~[a~<~I] \cdot \sigma$ & ~~~~ & (modal types)
  \end{tabular}
\end{center}

\subsection{Index terms and equational programs}

d$l$PCF is parameterized by a $\varepsilon$. $\varepsilon$ is called an equational
program and corresponds to a set of equations that define the values of index 
terms. Index terms are resolved to natural numbers but they are build according to 
the following grammar:

\begin{center}
$I,J,K::= a | f(I_{1},...,I_{\alpha(f)} | \sum_{a<I} J |\;\otriangleup_{a}^{I,J}K $
\end{center}

Where the first two constructions allow to define basic index variables and first order
functions that return index values (natural numbers). The third element corresponds to
a \emph{bounded sum}, which is mathematically just the sum of all possible values of J 
where $a$ is replaced by the values from 0 to $I-1$. Later, when explaining an example 
of the typing of a linear dependent type we'll see that bounded sums allow for example to 
aggregate all the substitutions that are made as part of all the recursive calls derived
from a function application. The last element is called \emph{forest cardinality}. The forest
cardinality is calculated by the method defined in the section 3.1 of \cite{ldtcv}, we won't
get into the details of how to calculate it but we're interested by its application to 
linear dependent types. Basically for a given number of trees that describe the calls, including
the recursive calls involved in the evaluation of a term, the forest cardinality allows to
define the number of recursive calls that are left when the evaluation is inside a given
level provided by a parameter. The parameters I and J represent respectively the level of the 
evaluation and the number of subtrees starting by the level I that will be evaluated. The 
utility of this concept will be demonstrated when explaining how to calculate the cost of
a recursive function like $add$ in the example studied.

\subsection{Typing additionnal features}


Most of the \emph{features} of the d$l$PCF type system where presented, but
there exists another one that can introduce the typing rules. Let's consider a
type $\sigma~=~[a~<~I] \cdot A\{a/x\}$, where, as explained before, is the type
where $a$ takes successively the values from 0 to I $-$ 1. Now, considering a
type $\tau~=~[b~<~J] \cdot A\{b+I/x\}$, an interesting proprety here is the fact
$b$ will take the values from $I$ to $J$, in the same type $A$, for the same value
$x$ that will be substituted. We can introduce the operator $\uplus$ on the
modal types, whose result will be:

\begin{center}
$\sigma~\uplus~\tau~\equiv~\alpha~=~[a~<~J] \cdot A\{a/x\}$
\end{center}

Formally, the binary operator $\uplus$ has the ability to merge two types giving
the linear and bounded types property. It is also possible to define an
operation of bounded sum on modal types. Imagine the following type :

\begin{center}
  $\tau~=~[b~<~J] \cdot A\{b~+~\sum_{d<a}J\{d/a\}/c\}$
\end{center}

Its bounded sum will then be : $\sum_{a<I} \tau ~=~ [c~<~\sum_{a<I}J] \cdot A$. 

Finally, the type Nat[I] is, of course, a syntactic sugar for Nat[I,I]

\subsection{Subtyping}

Of course, d$l$PCF, due to its linear and dependant nature, is subject to
subtyping. The inequality $\sqsubseteq$ can be used on types, with the rules
described in the Figure \label{subtyping}.

\begin{figure}
  \begin{center}
    \begin{tabular}{|c c|}
      \hline
      & \\
      \AxiomC{$\phi; \Phi \models~ K \leq I$}
      \AxiomC{$\phi; \Phi \models~ J \leq H$}
      \BinaryInfC{$\phi; \Phi \vdash Nat[I, J] \sqsubseteq Nat[K,H]$}
      \DisplayProof & 

      \AxiomC{$\phi; \Phi \vdash \sigma' \sqsubseteq \sigma$}
      \AxiomC{$\phi; \Phi \vdash \tau \sqsubseteq \tau'$}
      \BinaryInfC{
        $\phi; \Phi \vdash \sigma \multimap \tau \sqsubseteq \sigma'\multimap \tau'$}
      \DisplayProof \\ 
      & \\
      \multicolumn{2}{|c|}{
        \AxiomC{$(a,\phi); (a > J,\Phi) \vdash A \sqsubseteq B$}
        \AxiomC{$\phi; \Phi \vdash J \leq I$}
        \BinaryInfC{
          $\phi; \Phi \vdash \sigma \multimap [a < I] \cdot A \sqsubseteq [a < J]$
          $\cdot B $}
        \DisplayProof 
      } \\
      & \\
      \hline
    \end{tabular}
    \end{center}
  \caption{Subtyping rules}
  \label{subtyping}
\end{figure}

\subsection{Typing judgements}

We can now introduce the following form for typing judgements:

\begin{center}
  $\phi; \Phi; \Gamma \vdash_{K}^{\varepsilon} t~:~\tau$
\end{center}

The argument $K$ is the weight of $t$, in other word the maximal number of
substitution during its evaluation. $\Phi$ are the constraints envrionnement,
for example $(a~<~I)$ from $[a~<~I] \cdot Nat$, called the index context, and
$\Gamma$ is the typing context, associating a type to each free variable of $t$.
Finally, $\phi$ is a bounder for free variables contained in the other context
arguments.The sums and bounded sums can be applied over contexts. The derivation
rules are presented in the figure \ref{deriv-rules}.

\begin{figure}
  \centering
    \begin{tabular}{|c c|}
      \hline 
      & \\

      \AxiomC{$\phi; \Phi \vdash_{\varepsilon} \sigma \sqsubseteq \tau$}
      \RightLabel{(Ax)}
      \UnaryInfC{
        $\phi; \Phi; \Gamma, x \vdash_{I}^{\varepsilon} x : \tau$}
      \DisplayProof &

      \AxiomC{$(a, \phi); (a < I, \Phi); \Gamma, x$ 
        $\vdash_{K}^{\varepsilon} t : \tau$}
      \AxiomC{$\phi; \Phi \vdash_{\varepsilon} \Delta $
        $\sqsubseteq \sum_{a<I} \Gamma$}
      \RightLabel{$(\multimap)$}
      \BinaryInfC{
        $\phi; \Phi; \Gamma, x \vdash_{I}^{\varepsilon} x : \tau$}     
      \DisplayProof \\

      & \\

      \multicolumn{2}{|c|}{
        \AxiomC{$\phi; \Phi \models_{\varepsilon} K \leq I + 1$}
        \AxiomC{$\phi; \Phi \models_{\varepsilon} J + 1 \leq H$}
        \AxiomC{$\phi; \Phi; \Gamma \vdash_{M}^{\varepsilon}$
          $ t : Nat[I, J] $}
        \RightLabel{(s)}
        \TrinaryInfC{
          $\phi; \Phi; \Gamma \vdash_{M}^{\varepsilon}$
          $s(t) : Nat[K,H]$}
        \DisplayProof 
      } \\ 
      
      & \\
      
      \multicolumn{2}{|c|}{
        \AxiomC{$\phi; \Phi \models_{\varepsilon} I \leq n $}
        \AxiomC{$\phi; \Phi \models_{\varepsilon} n \leq J$}
        \RightLabel{(n)}
        \BinaryInfC{
          $\phi; \Phi; \Gamma \vdash_{0}^{\varepsilon}$
          $n : Nat[K,H]$}
        \DisplayProof 
      } \\ 
      
      & \\
      
      \multicolumn{2}{|c|}{
        \AxiomC{$\phi; (J \leq 0, \Phi); \Delta \vdash_{N}^{\varepsilon}$
          $ u : \tau$}
        \noLine
        \UnaryInfC{$\phi; (K \geq 1, \Phi); \Delta \vdash_{N}^{\varepsilon}$
          $ s : \tau$}
        \AxiomC{$\phi; \Phi; \Gamma \vdash_{N}^{\varepsilon} t : Nat[J,K]$}
        \noLine
        \UnaryInfC{$\phi; \Phi; \Gamma \vdash_{\varepsilon}$
          $ \Sigma \sqsupseteq \Gamma \uplus \Delta $}
        \RightLabel{(if)}
        \BinaryInfC{
          $\phi; \Phi; \Gamma \vdash_{M+N}^{\varepsilon}$
          $s(t) : Nat[K,H]$}
        \DisplayProof 
      } \\ 

      & \\
      
      %% Application
      \multicolumn{2}{|c|}{
        \AxiomC{$\phi; \Phi; \Gamma \vdash_{K}^{\varepsilon}$
          $ t : [a < J] \cdot \sigma \multimap \tau$}
        \noLine
        \UnaryInfC{$\phi; \Phi; \Delta \vdash_{H}^{\varepsilon}$
          $ u : \sigma \{0/a\}$}
        \noLine
        \UnaryInfC{$\phi; \Phi; \Delta \models_{\varepsilon}$
          $ J \leq 1 $}
        \AxiomC{$\phi; \Phi \vdash_{\varepsilon} \tau \{0/a\} \sqsubseteq \mu$}
        \noLine
        \UnaryInfC{$\phi; \Phi \vdash_{\varepsilon}$
          $ \Sigma \sqsupseteq \Gamma \uplus \Delta $}
        \RightLabel{(App)}
        \BinaryInfC{
          $\phi; \Phi; \Sigma \vdash_{K+H}^{\varepsilon}$
          $tu : \mu$}
        \DisplayProof 
      } \\ 
     
      & \\
      
      \multicolumn{2}{|c|}{
        \AxiomC{$(b,\phi); (b<H,\Phi); \Gamma, x :[a < I] \cdot A  $
          $ t : Nat[I, J] $}
        \noLine
        \UnaryInfC{$\phi; \Phi \models_{\varepsilon}$ 
          $H \ge \otriangleup_{b}^{0, K} $}
        \noLine
        \UnaryInfC{$(a,b,\phi); (a < I,b < H, \Phi) \vdash_{\varepsilon}$ 
          $B\{0/a\}\{\otriangleup_{b}^{b+1,a}I + b + 1 + 1/b\} \sqsubseteq A $}
        \noLine
        \UnaryInfC{$(a,\phi); (a < K, \Phi) \vdash_{\varepsilon}$ 
          $B\{0/a\}\{\otriangleup_{b}^{b+1,a}I/b\} \sqsubseteq C $}
        \noLine
        \UnaryInfC{$\phi; \Phi \vdash_{\varepsilon}$ 
          $\Sigma \sqsubseteq \sum_{b<H} \Gamma$}

        \RightLabel{(Rec)}
        \UnaryInfC{$\phi; \Phi; \Sigma \vdash^{\varepsilon}_{H+\sum_{b<H}J}$
          $fix~x.t~:~[a<K]~C$}
        \DisplayProof 
      } \\
      
     &\\

      \hline
      
    \end{tabular}

  \caption{Typing rules of d$l$PCF$_{V}$}
  \label{deriv-rules}.
\end{figure}

\section{An example of d$l$PCF$_{V}$}

The best way to understand all the concepts behind d$l$PCF$_{V}$ is to
explain them with an example. Let's take the add function defined by:

\begin{center}
$add~=~fix\;f.\lambda yz.\;ifz\;y\;then\;0\;else\;s(f\;p(y)\;z)$
\end{center}

Its dependent type is the following:

\begin{center}
$(Nat[f]\; =>\;(Nat[g]\; =>\;Nat[f + g]))$
\end{center}

Adding the annotations for linear dependent types in d$l$PCF$_{V}$:

\begin{center}
$\vdash^{\varepsilon}_{K}~add~:~[a~<~1] \cdot Nat[f]~\multimap~$
  $[c~<~1] \cdot (Nat[g]~\multimap~Nat[f + g])$
\end{center}

As explained previously, annotations like $[x~<~n]$ before a function
in d$l$PCF$_{V}$  indicate the number of times the function is copied in a term. Thus
$[a~<~1]$ and $[c~<~1]$ represent that both the partial function with
the fixpoint f and one argument and the fixpoint f with its two
arguments applied are copied only once in the term.

However what is more interesting about the add function is to try to
understand the process to calculate the cost of the function, also
called its weight, that is represented in the previous formula by the
term K. 

Calculating the cost for the term $t$ that is defined inside the
recursive function f (i.e $t\;=\lambda$
$yz.\;ifz\;y\;then\;0\;else\;s(f\;p(y)\;z)$) from an informal point of
view is straightforward. It reduces to counting how many times the
bounded variables y and z will be used every time the term t is
substituted. Since both y and z will be copied only once the weight of the
term t is 2.

The more interesting part in the process of calculating the weight of
add is replacing the parameter f by a recursive function that is
invoked as long as the first parameter it receives is bigger than
zero. The rule that defines typing for recursion is the following:

\begin{figure}
\caption{Typing rule for the fixpoint operator}

      \AxiomC{$(b,\phi); (b<H,\Phi); \Gamma, x :[a < I] \cdot A  $
        $ t : Nat[I, J] $}
        \noLine
      \UnaryInfC{$\phi; \Phi \models_{\varepsilon}$ 
      $H \ge \otriangleup_{b}^{0, K} $}
        \noLine
      \UnaryInfC{$(a,b,\phi); (a < I,b < H, \Phi) \vdash_{\varepsilon}$ 
      $B\{0/a\}\{\otriangleup_{b}^{b+1,a}I + b + 1 + 1/b\} \sqsubseteq A $}
        \noLine
      \UnaryInfC{$(a,\phi); (a < K, \Phi) \vdash_{\varepsilon}$ 
      $B\{0/a\}\{\otriangleup_{b}^{b+1,a}I/b\} \sqsubseteq C $}
        \noLine
      \UnaryInfC{$\phi; \Phi \vdash_{\varepsilon}$ 
      $\Sigma \sqsubseteq \sum_{b<H} \Gamma$}

      \RightLabel{(Rec)}
        \UnaryInfC{$\phi; \Phi; \Sigma \vdash^{\varepsilon}_{H+\sum_{b<H}J}$
        $fix~x.t~:~[a<K]~C$}
      \DisplayProof 

\end{figure}

By intuition the cost of a term with a recursive call is the sum of
the cost of all times the term is called recursively. The equational
context $\varepsilon$ allows us to calculate that cost. The context is
parameterized on b, the level of the recursion the execution is in. In 
$\varepsilon$ for the function add we define the following terms:


\begin{itemize}

\item \textbf{K} : represent the number of recursive calls.
\item \textbf{I} : is the number of times the recursive function will be called at
the level b. 
\item \textbf{H} : is calculated according to the formula explained in .... . It
represents the level of nesting.
\item \textbf{J} : is the number of copies of f. the first is not considered and
the last recursive call doesn't use it.

\end{itemize}

Figure~\ref{definitionindexterms} represents the definitions of the index
terms for the add function.


\begin{figure}[ht]
\begin{center}
  \begin{tabular}{l}
    $I~=~if~b~<~f~then~1~else~0$ \\

    $J~=~f~-~b~-~1$ \\

    $H~=~f~-~b$ \\

    $K~=~f~-~b~+~1$ \\
  \end{tabular}
\end{center}
\caption{Equational context for add function}
\label{definitionindexterms}
\end{figure}

The meaning of all the index terms defined previously can be better
understood by tracing the execution of a call to add. An example is
given at figure \ref{exectrace}. 


\begin{figure}
\label{exectrace}
\begin{center}
\begin{verbatim}
add 3 4 = fix f succ (f pred 3 4)
    add 2 4 = fix f succ (f pred 2 4)
        add 1 4 = fix f succ (f pred 1 4)
            add 0 4 = fix f 0 
\end{verbatim}
\end{center}

\caption{Execution trace of add 3 4}
\end{figure}

With the given formulas and the execution trace we can see how $I$ will always
be 1 unless $b~\geq~f$, that is $I$ will be 0 when the first parameter to add is
zero meaning that there will not be more recursive calls when the first
parameter has been fully decomposed. $I$ represents the number of substitutions of
the fixpoint function f


With the elements given by $\varepsilon$, calculating the cost of add
reduces to solving the $H + \sum\limits_{b<H} (2) $, where 2
represents the cost of the term $t$ previously explained.

%% TODO: Add the section that explains how to calculate <>0 to n
%% TODO: Finish this section


\section{Krivine's Abstract Machine and CEK}

The d$l$PCF can be interpreted by the KAM machine, but it is not really
efficient, and the article from Ugo Dal Lago \& Barbara Petit \cite{ldtcbv}
presents an alternative for the CEK abstract machine.

The CEK machine consists only on a closure and a stack, and few rules to
interact. A closure is a pair $v =<t;\xi>$, where $t$ is a value and $\xi$
an environment, in other terms a mapping from variables to value closures:

\begin{center}
  $\xi ~ ::=~\O~|~(x \mapsto v) \cdot \xi$
\end{center}

Stacks are terms from the following grammar:

\begin{center}
  $\pi~::=~\diamond~|~fun(v, \pi)~|~arg(c, \pi)$ \\
  $|~fork(t,u,\xi,\pi)~|~s(\pi)~|~p(\pi)$
\end{center}

Finally, a process $P$ is a pair $c~*~\pi$ of a closure and a stack. The process
evaluation is given in the figure \ref{cek-eval-closure}.

\begin{figure}[!ht]
  \begin{center}
    \begin{tabular}{|c c c c c c c|}
      \hline
      v & $*$ & $arg(c,~\pi)$ & $\succ$ & c & $*$ & $fun(v,~\pi)$ \\
      v & $*$ & $fun(<~\lambda x.t;\xi~>,~\pi$ & $\succ$ &
      $<~t;~(x~\mapsto~v)\cdot \xi~>$ 
      & $*$ & $\pi$ \\
      v & $*$ & $fun(<fix~x.t;~\xi>,~\pi>$ & $\succ$ &
      $<t;~(x\mapsto <fix~x.t;~\xi>) \cdot \xi>$ & $*$ & $arg(v,~\pi)$ \\
      $<0;~\xi'>$ & $*$ & $fork(t,~u,~\xi,~\pi)$ & $\succ$ &
      $<t;~\xi>$ & $*$ & $\pi$ \\
      $<n+1;~\xi'>$ & $*$ & $fork(t,~u,~\xi,~\pi)$ & $\succ$ &
      $<u;~\xi>$ & $*$ & $\pi$ \\
      $<n;~\xi>$ & $*$ & $s(\pi)$ & $\succ$ &
      $<n+1;~\O>$ & $*$ & $\pi$ \\
      $<n;~\xi>$ & $*$ & $p(\pi)$ & $\succ$ &
      $<n-1;~\O>$ & $*$ & $\pi$ \\
      \hline
    \end{tabular}
  \end{center}
  \caption{The CEK$_{PCF}$ closure evaluation rules}
  \label{cek-eval-closure}
\end{figure}


The PCF can be compiled, or evaluated, using the rules defined in the figure \ref{cek-eval-rules}

\begin{figure}[!ht]
  \begin{center}
    \begin{tabular}{|c c c c c c c|}
      \hline
      $<x;~\xi>$ & $*$ & $\pi$ & $\succ$ & $\xi(\pi)$ & $*$ & $\pi$ \\
      $<tu;~\xi>$ & $*$ & $\pi$ & $\succ$ & $<t;~\xi>$ & $*$ & 
      $arg(<u;~\xi>,~\pi)$ \\
      $<s(t);~\xi>$ & $*$ & $\pi$ & $\succ$ & $<t;~\xi>$ & $*$ & 
      $s(\pi)$ \\
      $<p(t);~\xi>$ & $*$ & $\pi$ & $\succ$ & $<t;~\xi>$ & $*$ & 
      $p(\pi)$ \\
      $ifz~t~then~u~else~s;~\xi$ 
      & $*$ & $\pi$ & $\succ$ & $<t;~\xi>$ & $*$ & 
      $fork(t,~u,~\xi,~\pi)$ \\
      \hline
    \end{tabular}
  \end{center}
  \caption{The CEK$_{PCF}$ evaluation rules}
  \label{cek-eval-rules}
\end{figure}

\section{Conclusion}

Linear and dependent types can be really powerful and give some informations
that can be really useful, like the termination of a program or its
computational cost.


\bibliographystyle{plain}
\bibliography{bibliography}

\end{document}



% Local Variables:
% compile-command: "rubber -d notes.tex"
% ispell-local-dictionary: "english"
% End:
