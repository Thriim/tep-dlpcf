\documentclass[a4paper,12pt]{report}

\usepackage{amssymb}
\usepackage[utf8]{inputenc}
%% \usepackage[english]{babel}
\usepackage{verbatim}

\title{Linear Dependant Types in a Call-by-Value Scenario \\
- \\
Personal notes}

\author{Pierrick COUDERC}

\begin{document}

\maketitle

\chapter{Notes}

\section{Introduction}

\section{Linear Dependent Types, Intuitively}

Lambda-calculus in PCF Type System: 
\begin{itemize}
\item fix: fixpoint operator, for recursive functions
\item ifz: if zero
\item s: succ
\item p: pred
\end{itemize}

Functions presented:
\begin{verbatim}
let rec dbl x =
  if x = 0 then x
  else succ @@ succ @@ dbl @@ pred x

let rec div x = (* diverges *)
  if x = 0 then x
  else succ @@ succ @@ div x
\end{verbatim}


\chapter{Article}

\paragraph{Abstract}

\section{Introduction}

Types and types systems are an extremely useful and powerful tools
that warranty that a program verifies certain properties, mainly that
it ``cannot go wrong'' (i.e. there shouldn't be types
incompatibilities and crash during the execution), especially with
languages using strong type systems. However, most types systems
aren't capable to catch other properties like termination and the
execution time, i.e. the computational complexity. For example, we can
consider this simple factorial function:

\begin{verbatim}
let rec fact n = if n = 0 then 1 else n * fact (n-1)
\end{verbatim}

We can easily prove that such a recursive function will terminate if we consider
only the natural integers, but there is no other way to check it before
executing the function.

The idea of the linear dependent types is to extend a type system to
add the ability for the typechecker to, given a program: verify that
it terminates, give an approximation of its computational complexity and
specify the semantics of its behavior. Plotkin's PCF, which is a
lambda calculus extended with natural integers, will be used as a
language and types (only Nat and functions on Nat actually), adding
the two following properties :

\begin{itemize}
\item \textbf{Linearity} : being able to catch the number of evaluation required
  for a term t, this way giving an idea of the computational complexity the
  program.
\item \textbf{Dependency} : the possibility to instantiate distinct copies of a term with
  different types.
\end{itemize}

Such PCF programs, typed with dlPCF, can then be interpreted by Krivine's
Abstract Machine for a Call-by-Name evaluation strategy, but this setting is not
really efficient, and few languages are actually using this strategy. the idea
is then to extend the original dlPCF (called $ dlPCF_{N} $) with the ability to type
programs with a call-by-value strategy, namely $ dlPCF_{v} $, and use the Felleisen and
Freidman's CEK machine to interpret it.

\section{Introduction to linear and dependent types}

Linear and dependent types are two completely different
concepts. Linear types come from Girard's linear logic
\cite{girard1987linear} a really powerful extension of classical logic
that expresses well resource manipulation. It does so by controlling
the use of the rules of contraction and weakening two rules that don't
fit in a system where proposition do not represent unlimited
resources.

\section{Linear Dependent Types}

\begin{figure}
  \begin{center}
    $t~::=~n~|~x~|~(\lambda x.t)~|~p(t)~|~s(t)$ \\
    $|~ifz~$t$~then~$u$~else~$v $|~fix~x.t$
  \end{center}
  \caption{Plotkin's PCF's syntax}
  \label{pcf-syntax}
\end{figure}

\begin{figure}
  \begin{center}
    $(\lambda x.t)~v~\rightarrow~ t[v/x]$ \\
    $s(n)~\rightarrow~ n~+~1$ \\
    $p(n+1)~\rightarrow~ n$ \\
    $p(0)~\rightarrow~ 0$ \\
    $ifz~0~then~u~else~v~\rightarrow~u$ \\
    $ifz~n+1~then~u~else~v~\rightarrow~v$ \\
    $(fix~x.t)~v~\rightarrow~(t[(fix~x.t)/x])~v$
  \end{center}
  \caption{The PCF semantics}
  \label{pcf-sem}
\end{figure}

First of all, let's consider a simple program written in PCF (whose syntax and
semantic are recalled in Figures \ref{pcf-syntax} and \ref{pcf-sem} :

\begin{center} 
  dbl = fix~f.$\lambda $x.~ifz~x~then~x~else~s(s(f(p(x)))) 
\end{center}

Simply, this function will return the double of its argument. In the PCF type
system, it will be typed as Nat $\rightarrow$ Nat. However, there are no other
information about its behavior and its meaning in this type, since, for example,
a lot of unary functions can be typed the same way (an identity function for
example). In particular, if we change p(x) by x, this function will have the
same type, but it will diverge. We may need a type system that will be able to
catch those diverging behavior, if that's ever possible.

First of all, we can extend the types with informations on values, like for
\emph{dbl} : $\tau$ = Nat[a] $\rightarrow$ Nat[2 $\times$ a]. This way, we can
see that the type contains the exact behavior of the function, however it will
be rarely possible to specify such a precise type. it is necessary then to add a
\emph{imprecise type}, like for example \textbf{Nat[2, 4]} which is a natural
integer whose value is between 2 and 4.

\medskip

Secondly, a linear dependent type system has an interesting proprety, which
captures the termination behaviour for functions that are fully recursive. An
good example was the one we provided before, but in a classic type system, their
behaviour cannot be differentiated. However, one terminates, while the other
diverges. This is a property that sized types and dependent types satisfies. To
do so, the type system, by trying to prove termination, could also gives the
time and space consumption. Using the ideas from linear logic, we could, for
example give type the following way :

\begin{center}
$\vdash_{I}$ dbl : Nat $\rightarrow$ Nat
\end{center}

In this type, I is the cost to compute dbl, and, since it can be given, returns
the fact that the function doesn't diverge.

The idea behind dlPCF merges the imprecise types and the computational costs,
combined with ideas from the bounded linear logic. This one allows a practical
way to represent the number of times a function uses its argument, with the
following syntax as instance :

\begin{center}
$!_{n}\sigma~\multimap~\tau$
\end{center} 

This would mean that $\sigma$ uses its argument $n$ times during evaluation. 

\medskip

Finally, a judgement in dlPCF would look this way, for a function from Nat to
Nat :

\begin{center}
$\vdash_{g(a)}~t~:~!_{m}~Nat[a]~\multimap~Nat[f(a)]$ 
\end{center}

In such a type, the function would use its argument $m$ times, returning 
the value of $f$ applied to its argument, at a computational cost of
$g(a)$. Actually, d$l$PCF is slightly improved by changing the $!_{m}\sigma$ for
$[a < m] \cdot \sigma$, which makes the type more parametric, could serves as a
variable bounder and allows multiple variables.

At last, there another parameter that con be considered, which is the $\varepsilon$
class of functions that can be used to tune the type system and cost of
evaluation.

Finally, we can type \emph{dbl} in d$l$PCF:

\begin{center}
$\vdash^{\varepsilon}_{h(a)}~dbl~:~[b~<~a] \cdot Nat[a]~\multimap~Nat[2~\times~a]$
\end{center}

\section{d$l$PCF}


Of course, programs that will be considered will be written in PCF, which is an
extension of the $\lambda$-calculus. 

More formally, the types will be defined as follows:

\begin{center}
  $\sigma, \tau~::=~Nat[I,J]~|~A~\multimap~\sigma$~~~~~(basic types) \\
  $A, B~::=~[a~<~I] \cdot \sigma$~~~~~(modal types)
\end{center}

\subsection{Typing additionnal features}


Most of the \emph{features} of the d$l$PCF type system where presented, but
there exists another one that can introduce the typing rules. Let's consider a
type $\sigma~=~[a~<~I] \cdot A\{a/x\}$, where, as explained before, is the type
where $a$ takes successively the values from 0 to I $-$ 1. Now, considering a
type $\tau~=~[b~<~J] \cdot A\{b+I/x\}$, an interesting proprety here is the fact
$b$ will take the values from $I$ to $J$, in the same type $A$, for the same value
$x$ that will be substituted. We can introduce the operator $\uplus$ on the
modal types, whose result will be:

\begin{center}
$\sigma~\uplus~\tau~\equiv~\alpha~=~[a~<~J] \cdot A\{a/x\}$
\end{center}

Formally, the binary operator $\uplus$ has the ability to merge to types giving
the linear and bounded types property. It is also possible to define an
operation of bounded sum on modal types. Imagine the following type :

\begin{center}
  $\tau~=~[b~<~J] \cdot A\{b~+~\sum_{d<a}J\{d/a\}/c\}$
\end{center}

Its bounded sum will then be : $\sum_{a<I} \tau ~=~ [c~<~\sum_{a<I}J] \cdot A$. 

Finally, the type Nat[I] is, of course, a syntactic sugar for Nat[I,I]

\subsection{Subtyping}

Of course, d$l$PCF, due to its linear and dependent nature, is subject to
subtyping. The inequality $\sqsubseteq$ can be used on types, with the rules
described in the Figure \label{subtyping}.

\begin{figure}
  (Here will be the subtyping rules) \\
  $\phi , \Phi$
  \caption{Subtyping rules}
  \label{subtyping}
\end{figure}

\section{Krivine's Abstract Machine and CEK}

The d$l$PCF can be interpreted by the KAM machine, but it is not really
efficient, and the article from [Ugo Dal Lago \& Barbara Petit] presents a
alternative for the CEK abstract machine.

The CEK machine consists only on a closure and a stack, and few rules to
interact. A closure is a pair $v =<t;\xi>$, where $t$ is a value and $\xi$
an environment, in other term a mapping from variables to value closures:

\begin{center}
  $\xi ~ ::=~\O~|~(x \mapsto v) \cdot \xi$
\end{center}

Stacks are terms from the following grammar:

\begin{center}
  $\pi~::=~\diamond~|~fun(v, \pi)~|~arg(c, \pi)$ \\
  $|~fork(t,u,\xi,\pi)~|~s(\pi)~|~p(\pi)$
\end{center}

\end{document}

% Local Variables:
% compile-command: "rubber -d notes.tex"
% ispell-local-dictionary: "english"
% End:
